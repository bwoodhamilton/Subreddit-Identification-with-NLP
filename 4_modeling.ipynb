{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experimentation \n",
    "\n",
    "### Contents:\n",
    "- [1. Multinomial Naive Bayes and Count Vectorizer](#1.-Multinomial-Naive-Bayes-and-Count-Vectorizer)\n",
    "- [2. Multinomial Naive Bayes and TFIDF Vectorizer](#2.-Multinomial-Naive-Bayes-and-TFIDF-Vectorizer)\n",
    "- [3. Logisitic Regression with Count Vectorizer](#3.-Logistic-Regression-with-Count-Vectorizer)\n",
    "- [4. Logisitic Regression with TFIDF Vectorizer](#4.-Logistic-Regression-with-TFIDF-Vectorizer)\n",
    "- [5. KNN and Count Vectorizer](#5.-KNN-and-Count-Vectorizer)\n",
    "- [6. KNN with TFIDF Vectorizer](#6.-KNN-and-TFIDF-Vectorizer)\n",
    "- [7. Decision Tree with Count Vectorization](#7.-Decision-Tree-with-Count-Vectorization)\n",
    "- [8. Decision Tree with TFIDF Vectorizer](#8.-Decision-Tree-with-TFIDF-Vectorizer)\n",
    "- [9. Bagging Classifier with TFIDF Vectorizer](#9.-Bagging-Classifier-with-TFIDF-Vectorizer)\n",
    "- [10. Random Forest with TFIDF Vectorizer](#10.-Random-Forest-with-TFIDF-Vectorizer)\n",
    "- [11. Extra Trees with TFIDF Vectorizer](#11.-Extra-Trees-with-TFIDF-Vectorizer)\n",
    "- [12. AdaBoostClassifier and TfidfVectorizer](#12.-AdaBoostClassifier-and-TfidfVectorizer)\n",
    "- [13. Gradient Boosting and TfidfVectorizer](#13.-Gradient-Boosting-and-TfidfVectorizer)\n",
    "- [14. SVM and TfidfVectorizer](#14.-SVM-and-TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The long list of packages I need to run all my models\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in my proprocessed csv to pandas\n",
    "reddits = pd.read_csv('./data/reddits_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>type</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>comment</td>\n",
       "      <td>1553281124</td>\n",
       "      <td>she didnt mention this when i asked her  she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>comment</td>\n",
       "      <td>1553280963</td>\n",
       "      <td>i mean i was but not for the sole purpose of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>comment</td>\n",
       "      <td>1553280896</td>\n",
       "      <td>hardly the best talent around in podcast has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>comment</td>\n",
       "      <td>1553280716</td>\n",
       "      <td>she cant do season 2 because gimlet owns the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>comment</td>\n",
       "      <td>1553280571</td>\n",
       "      <td>search party really excellent and critically...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit     type  created_utc  \\\n",
       "0          0  comment   1553281124   \n",
       "1          0  comment   1553280963   \n",
       "2          0  comment   1553280896   \n",
       "3          0  comment   1553280716   \n",
       "4          0  comment   1553280571   \n",
       "\n",
       "                                               words  \n",
       "0    she didnt mention this when i asked her  she...  \n",
       "1    i mean i was but not for the sole purpose of...  \n",
       "2    hardly the best talent around in podcast has...  \n",
       "3    she cant do season 2 because gimlet owns the...  \n",
       "4    search party really excellent and critically...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out the data\n",
    "reddits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28012, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of my data\n",
    "reddits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning my X and y variables. X is the words used to predict which subreddit and y is which subreddit\n",
    "X = reddits['words']\n",
    "y = reddits['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split my data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Multinomial Naive Bayes and Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(...\n",
       "                                              frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a pipeline for Multinomial Naive Bayes and CountVectorizer\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('nb', MultinomialNB())\n",
    "                ])\n",
    "# Setting pipeline parameters \n",
    "pipe_params = {\n",
    "    'vect__max_features': [100, 1000, 10000],\n",
    "    'vect__ngram_range': [(1,1), (1,2)],\n",
    "    'vect__stop_words' : [None, stop_words.ENGLISH_STOP_WORDS],\n",
    "}\n",
    "# Instantiating my gridsearch\n",
    "gs = GridSearchCV(pipe, \n",
    "                  param_grid=pipe_params\n",
    "                 ) \n",
    "# Fit GridSearch to training data.\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7936597788813231\n"
     ]
    }
   ],
   "source": [
    "# Printing the best score \n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the best parameters for this model. Commented this out only because it takes up so much space\n",
    "# and it's a lot of scrolling\n",
    "# gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting my best estimator to be my model for scoring\n",
    "gs_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8369746299205103"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training score\n",
    "gs_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.790946737112666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing score\n",
    "gs_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "Naive Bayes models are known to be good models for Natural Language Processing. They are probabilistic algorithms that predict the tag of a text. The model is naive because it assumes no relationship between features. This model was using the count vectorizer which generally was outperformed by the TFIDF vectorizer because the TFIDF vectorizer gives more weight to words that are unique to the document rather than just counting each word. The best parameters were max_features': 10000, ngram_range: (1, 1), and stop words included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multinomial Naive Bayes and TFIDF Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                                               frozenset({'a', 'about', 'above',\n",
       "                                                          'across', 'after',\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a pipeline\n",
    "pipe2 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('nb', MultinomialNB())\n",
    "                ])\n",
    "# Setting the parameters of the pipeline \n",
    "pipe_params2 = {\n",
    "    'tfidf__max_features': [100, 1000, 10000],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__stop_words' : [None, stop_words.ENGLISH_STOP_WORDS],\n",
    "}\n",
    "# Instantiated the grid search\n",
    "gs2 = GridSearchCV(pipe2, \n",
    "                  param_grid=pipe_params2\n",
    "                 ) \n",
    "# Fitting the model\n",
    "gs2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the best score\n",
    "gs2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the best parameters for the gridsearch\n",
    "# gs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator as the model\n",
    "gs_model2 = gs2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8587272121471751"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7948022276167357"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model2 = gs2.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "As stated above, Naive Bayes models are great models for Natural Language Processing, and using the TFIDF vectorizer improved its performance. This was the best model, although the SVM model was also very accurate and too close to call. The best parameters were the same as above, max_features': 10000, ngram_range: (1, 1), and stop words included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logisitic Regression with Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(...\n",
       "                                              frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting a pipeline including scaling my features\n",
    "pipe3 = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('scaler',  StandardScaler(with_mean=False)),\n",
    "                     ('lr', LogisticRegression())\n",
    "                ])\n",
    "# Setting pipeline parameters. The main issue I had with this model was that I could not set a large number\n",
    "# of max features or I would get an error code. I had to limit the features to 200 for max_features for \n",
    "# this code to run without errors. \n",
    "pipe_params3 = {\n",
    "    'vect__max_features': [100, 200],\n",
    "    'vect__ngram_range': [(1,1), (1,2)],\n",
    "    'vect__stop_words' : [None, stop_words.ENGLISH_STOP_WORDS],\n",
    "}\n",
    "# Instantiated a grid search\n",
    "gs3 = GridSearchCV(pipe3, \n",
    "                  param_grid=pipe_params3) \n",
    "# Fitting the model\n",
    "gs3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6879432618488764"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score\n",
    "gs3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the parameters\n",
    "# gs3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator as this model\n",
    "gs_model3 = gs3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6998905231091437"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score\n",
    "gs_model3.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.697129801513637"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score\n",
    "gs_model3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "The logistic regression model was one of the weakest, and it could not handle 10,000 features like I had in the previous models. The best parameters were max_features': 200, ngram_range: (1, 1), and stop words included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logisitic Regression with TFIDF Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                                               frozenset({'a', 'about', 'above',\n",
       "                                                          'across', 'after',\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a pipeline, same as above but with TfidfVectorizer \n",
    "pipe4 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                  ('scaler',  StandardScaler(with_mean=False)),\n",
    "                     ('lr', LogisticRegression())\n",
    "                ])\n",
    "# Setting pipeline parameters. I could not use more than 100 features without getting an error\n",
    "pipe_params4 = {\n",
    "    'tfidf__max_features': [100],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__stop_words' : [None, stop_words.ENGLISH_STOP_WORDS],\n",
    "}\n",
    "# Instantiated grid search\n",
    "gs4 = GridSearchCV(pipe4, \n",
    "                  param_grid=pipe_params4) \n",
    "# Fitting the model\n",
    "gs4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6591935171936693"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score\n",
    "gs4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the best parameters\n",
    "# gs4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator to be the model\n",
    "gs_model4 = gs4.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6635251558855728"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score\n",
    "gs_model4.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6538626303012994"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score \n",
    "gs_model4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "This model was even worse than the previous one in spite of using TFIDF Vectorizer, which worked better on the other models. I believe this is because I could only set the max features to 100 before I started to get errors which prevented it from running. The best parameters were max_features: 100, ngram_range: (1, 1), and stop words included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. KNN and Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(...\n",
       "                         'vect__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the pipeline including scaling features\n",
    "pipe5 = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('scaler',  StandardScaler(with_mean=False)),\n",
    "                     ('knn', KNeighborsClassifier())\n",
    "                ])\n",
    "# Setting the pipe parameters \n",
    "pipe_params5 = {\n",
    "    'vect__max_features': [100, 10000],\n",
    "    'vect__ngram_range': [(1,1), (1,2)],\n",
    "    'vect__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'knn__n_neighbors': [3, 5, 7]\n",
    "}\n",
    "# Instantiated a grid search\n",
    "gs5 = GridSearchCV(pipe5, \n",
    "                  param_grid=pipe_params5) \n",
    "# Fitting the model\n",
    "gs5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6579561358716409"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best score from the model\n",
    "gs5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best parameters\n",
    "# gs5.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to the best estimator \n",
    "gs_model5 = gs5.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8242657908515398"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training score\n",
    "gs_model5.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6621447950878195"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing score \n",
    "gs_model5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "Based on the training and testing scores, this model is overfit. The best parameters were max_features: 10,000, ngram_range: (1, 1), stop words included, and 3 n_neighbors for the KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. KNN with TFIDF Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                         'tfidf__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                          'across', 'after',\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting a pipeline for knn and tfidf\n",
    "pipe6 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                  ('scaler',  StandardScaler(with_mean=False)),\n",
    "                     ('knn', KNeighborsClassifier())\n",
    "                ])\n",
    "# Setting the pipeline parameters \n",
    "pipe_params6 = {\n",
    "    'tfidf__max_features': [100, 10000],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'knn__n_neighbors' : [3, 5, 7]\n",
    "}\n",
    "# Instantiated the grid search\n",
    "gs6 = GridSearchCV(pipe6, \n",
    "                  param_grid=pipe_params6) \n",
    "# Fitting the model\n",
    "gs6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the best score\n",
    "gs6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the best parameters\n",
    "# gs6.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to be the best estimator \n",
    "gs_model6 = gs6.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6777571516968918"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The training score of the model\n",
    "gs_model6.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6095958874767956"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The testing score of this model \n",
    "gs_model6.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "Unlike most of the other models, this one performed better with the count vectorizer than the tfidf vectorizer. This model was also overfit, but not as much as the previous one with the count vectorizer. The best parameters were max_features: 100, ngram_range: (1, 1), stop words included, and for knn, n_neighbors of 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Decision Tree with Count Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(...\n",
       "                         'vect__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the pipeline for decision tree and count vectorizer \n",
    "pipe7 = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('dt', DecisionTreeClassifier())\n",
    "                ])\n",
    "# Setting the pipe parameters\n",
    "pipe_params7 = {\n",
    "    'vect__max_features': [10000],\n",
    "    'vect__ngram_range': [(1,1)],\n",
    "    'vect__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'dt__max_depth': [3, 10],\n",
    "    'dt__min_samples_split': [5, 20],\n",
    "    'dt__min_samples_leaf': [2, 7]\n",
    "}\n",
    "# Instantiated a grid search\n",
    "gs7 = GridSearchCV(pipe7, \n",
    "                  param_grid=pipe_params7) \n",
    "# Fitting the model\n",
    "gs7.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6237326146026518"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score\n",
    "gs7.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the best parameters\n",
    "# gs7.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator as the model \n",
    "gs_model7 = gs7.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6251606454376696"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out the training score\n",
    "gs_model7.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6320148507782379"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out the testing score \n",
    "gs_model7.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "This model was one of the worst performing ones on this particular dataset. The accuracy scores for the training and testing data are only about 10% more than the baseline. The best parameters were max_features: 100, ngram_range: (1, 1), stop words included, and for the decision tree, max_depth: 10, min_samples_leaf: 2, min_samples_split: 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Decision Tree with TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                         'tfidf__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                          'across', 'after',\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up a pipeline for Decision tree and tfidf Vectorizer\n",
    "pipe8 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('dt', DecisionTreeClassifier())\n",
    "                ])\n",
    "# I removed tfidf feature options so I could try more dt hyperparameters since there has been a lot of\n",
    "# consistency with hyperparametes that work best\n",
    "pipe_params8 = {\n",
    "    'tfidf__max_features': [10000],\n",
    "    'tfidf__ngram_range': [(1,1)],\n",
    "    'tfidf__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'dt__max_depth': [3, 10],\n",
    "    'dt__min_samples_split': [5, 20],\n",
    "    'dt__min_samples_leaf': [2, 7]\n",
    "}\n",
    "# Instantiated grid search\n",
    "gs8 = GridSearchCV(pipe8, \n",
    "                  param_grid=pipe_params8) \n",
    "# Fitting the model\n",
    "gs8.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6273975360686205"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score\n",
    "gs8.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing what the best parameters are \n",
    "# gs8.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator to be the model\n",
    "gs_model8 = gs8.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6320624494264363"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score for this model \n",
    "gs_model8.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6375838926174496"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score for this model \n",
    "gs_model8.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "This model only did slightly better than with the Count Vectorizer. The best parameters were max_features: 100, ngram_range: (1, 1), stop words included, and for decision tree classifier max_depth: 10, min_samples_leaf: 7, min_samples_split: 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Bagging Classifier with TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                         'tfidf__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                          'across', 'after',\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the pipeline for a bagging classifier \n",
    "pipe9 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('bag', BaggingClassifier())\n",
    "                ])\n",
    "# Setting the parameters\n",
    "pipe_params9 = {\n",
    "    'tfidf__max_features': [10000],\n",
    "    'tfidf__ngram_range': [(1,1)],\n",
    "    'tfidf__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'bag__max_samples' : [.5, 1.0, 10],\n",
    "    'bag__n_estimators' : [2, 6, 10]\n",
    "}\n",
    "# Instantiated the grid search\n",
    "gs9 = GridSearchCV(pipe9, \n",
    "                  param_grid=pipe_params9) \n",
    "# Fitting the model to the data\n",
    "gs9.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7232135523137042"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score\n",
    "gs9.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the best parameters\n",
    "# gs9.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to the best estimator \n",
    "gs_model9 = gs9.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9658717692417536"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score\n",
    "gs_model9.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7256889904326718"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score \n",
    "gs_model9.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "As we discussed in class, certain models including the Bagging Classifier can become overfit very quickly. This is evident with this model, since there is a large disparity between the training and testing scores. Also, at this point I decided to only use the TFIDF Vectorizor since it was performing better than the Count Vectorizer on most models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Random Forest with TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                         'tfidf__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                          'across', 'after',\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the pipeline for random forest \n",
    "pipe10 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('rf', RandomForestClassifier())\n",
    "                ])\n",
    "# Pipeline parameters\n",
    "pipe_params10 = {\n",
    "    'tfidf__max_features': [10000],\n",
    "    'tfidf__ngram_range': [(1,1)],\n",
    "    'tfidf__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'rf__n_estimators': [100, 150],\n",
    "    'rf__max_depth': [None, 5, 6]\n",
    "}\n",
    "# Instantiating a grid search\n",
    "gs10 = GridSearchCV(pipe10, \n",
    "                  param_grid=pipe_params10) \n",
    "# Fitting my model\n",
    "gs10.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7564375042274222"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best score for this model\n",
    "gs10.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best parameters for this model\n",
    "# gs10.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator as this model\n",
    "gs_model10 = gs10.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9809605407206435"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score\n",
    "gs_model10.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7528202199057547"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score \n",
    "gs_model10.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "As with the Bagging model, Random Forest models have a tendancy to be very overfit and this is an example of that. While it is overfit, the accuracy on the testing data is still near the higher end of the results for testing. The best parameters were max_features: 100, ngram_range: (1, 1), stop words included, and for decision tree classifier rf__max_depth: None, and n_estimators: 150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Extra Trees with TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})],\n",
       "                         'xt__max_depth': [None, 5, 6],\n",
       "                         'xt__n_estimators': [100, 150]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the pipeline for tfidf and extra trees\n",
    "pipe11 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('xt', ExtraTreesClassifier())\n",
    "                ])\n",
    "# Setting the pipeline parameters\n",
    "pipe_params11 = {\n",
    "    'tfidf__max_features': [10000],\n",
    "    'tfidf__ngram_range': [(1,1)],\n",
    "    'tfidf__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'xt__n_estimators': [100, 150],\n",
    "    'xt__max_depth': [None, 5, 6]\n",
    "}\n",
    "# Instantiating the grid search\n",
    "gs11 = GridSearchCV(pipe11, \n",
    "                  param_grid=pipe_params11) \n",
    "# Fitting the model\n",
    "gs11.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7690991843582039"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score \n",
    "gs11.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the best parameters \n",
    "# gs11.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator as the model \n",
    "gs_model11 = gs11.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9809605407206435"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score \n",
    "gs_model11.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.776095958874768"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score\n",
    "gs_model11.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "Like the the other tree-based models, this one also was very overfit, however, the accuracy score on the testing data was still near the top of the list of models. The best parameters were max_features: 100, ngram_range: (1, 1), stop words included, and for Extra Trees max_depth: None and n_estimators: 150. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. AdaBoostClassifier and TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=10000,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=frozenset(...\n",
       "                                                        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('ada',\n",
       "                                        AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                           base_estimator=None,\n",
       "                                                           learning_rate=1.0,\n",
       "                                                           n_estimators=50,\n",
       "                                                           random_state=None))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'tfidf__max_df': (0.25, 0.5, 0.75)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the pipeline \n",
    "pipe12 = Pipeline([('tfidf', TfidfVectorizer(max_features=10000, \n",
    "                                           ngram_range=(1, 1), \n",
    "                                           stop_words=stop_words.ENGLISH_STOP_WORDS)),\n",
    "                     ('ada', AdaBoostClassifier()),\n",
    "])\n",
    "# Setting the pipeline parameters\n",
    "pipe_params12 = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "}\n",
    "# Instantiated a grid search\n",
    "gs12 = GridSearchCV(pipe12, \n",
    "                  param_grid=pipe_params12) \n",
    "# Fitting the model\n",
    "gs12.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6912749406574735"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the best score for the model \n",
    "gs12.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__max_df': 0.25}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best parameters \n",
    "gs12.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator to be the model\n",
    "gs_model12 = gs12.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6998905231091437"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score\n",
    "gs_model12.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6981293731258033"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score\n",
    "gs_model12.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "This model was not one of the stronger models, so it seems like boosting in general is not the best strategy for this data. I also tried a new parameter for TFIDF, and the best one for max_df was .25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Gradient Boosting and TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=10000,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=frozenset(...\n",
       "                                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                                   n_estimators=100,\n",
       "                                                                   n_iter_no_change=None,\n",
       "                                                                   presort='deprecated',\n",
       "                                                                   random_state=None,\n",
       "                                                                   subsample=1.0,\n",
       "                                                                   tol=0.0001,\n",
       "                                                                   validation_fraction=0.1,\n",
       "                                                                   verbose=0,\n",
       "                                                                   warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'tfidf__max_df': (0.25, 0.5, 0.75)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a pipeline\n",
    "pipe13 = Pipeline([('tfidf', TfidfVectorizer(max_features=10000, \n",
    "                                           ngram_range=(1, 1), \n",
    "                                           stop_words=stop_words.ENGLISH_STOP_WORDS)),\n",
    "                     ('gbc', GradientBoostingClassifier()),\n",
    "])\n",
    "# Setting the pipeline parameters \n",
    "pipe_params13 = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "}\n",
    "# Instantiated a grid search pipeline \n",
    "gs13 = GridSearchCV(pipe13, \n",
    "                  param_grid=pipe_params13) \n",
    "# Fitting the model\n",
    "gs13.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.697605588116698"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the best score \n",
    "gs13.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator to the model\n",
    "gs_model13 = gs13.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7138845256794707"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score \n",
    "gs_model13.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7061259460231329"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score \n",
    "gs_model13.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "This model did not perform strongly, and it seems that boosting is not a particularly effective model for this data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. SVM and TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=10000,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=frozenset(...\n",
       "                                            cache_size=200, class_weight=None,\n",
       "                                            coef0=0.0,\n",
       "                                            decision_function_shape='ovr',\n",
       "                                            degree=3, gamma='scale',\n",
       "                                            kernel='rbf', max_iter=-1,\n",
       "                                            probability=False,\n",
       "                                            random_state=None, shrinking=True,\n",
       "                                            tol=0.001, verbose=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'tfidf__max_df': (0.25, 0.5, 0.75)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the pipeline, this time just putting the features for TFIDF that have been most successful in the\n",
    "# pipeline rather than in the parameters. \n",
    "pipe14 = Pipeline([('tfidf', TfidfVectorizer(max_features=10000, \n",
    "                                           ngram_range=(1, 1), \n",
    "                                           stop_words=stop_words.ENGLISH_STOP_WORDS)),\n",
    "                     ('svc', SVC(gamma='scale')),\n",
    "])\n",
    "# Setting the parameters\n",
    "pipe_params14 = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "}\n",
    "# Instantiating a grid search pipeline\n",
    "gs14 = GridSearchCV(pipe14, \n",
    "                  param_grid=pipe_params14) \n",
    "# Fitting the model\n",
    "gs14.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7906613314003227"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score\n",
    "gs14.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator to the model\n",
    "gs_model14 = gs14.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9611119044219144"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score\n",
    "gs_model14.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7996572897329716"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score \n",
    "gs_model14.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "The SVM model was one of the strongest. It was basically neck in neck with Multinomial Naive Bayes. However, the downside to this model seems to be that it is very taxing on my computer's resources, so I did not dare run this with too many hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusions \n",
    "\n",
    "Of the models I had the chance to run, the best models for this type of data seem to be Naive Bayes and SVM. The accuracy results were very close for each. However, the downside to SVM is that it seems very prone to overfitting and seems to be very taxing on computer resources, as it took a long time to run each time I ran it. For this type of data, in the future I would choose Multinomial Naive Bayes because it was the most efficient with the most accuracy. I am going to do some tweaking to both models and try both with authors to see what I get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
